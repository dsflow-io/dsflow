version: '0.1'

pipeline:
  - dump_csv_file
  - create_table
  - data_checks

schedule:
  cron: daily
  dependencies: null
  partition_type: ds

datasets:
  raw_{{ flow_name }}:
    type: raw
    path: "datastore:/raw/{{ flow_name }}/"

  {{ flow_name }}:
    type: dim_table  # or fct_table
    path: "datastore:/tables/{{ flow_name }}/"

tasks:  # or use a Airflow DAG
  dump_csv_file:
    description: Dump {{ flow_name }} from url to datastore
    type: sh
    environment: base  # not implemented
    parameters:
      source: "{{ source_url }}"
      sink: raw_{{ flow_name }}
      authentication: null

  create_table:
    description: Convert raw_{{ flow_name }} into parquet
    type: notebook
    script: {{ flow_name }}.ipynb
    environment: pyspark-2.2  # not implemented
    parameters:
      source: raw_{{ flow_name }}
      sink: {{ flow_name }}

  data_checks:
    description: Make sure the csv file is sane
    sources:
      - raw_{{ flow_name }}
      - {{ flow_name }}
