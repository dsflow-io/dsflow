# <markdowncell>

# *Note: this notebook was generated by dsflow*
#
# # {{ flow_name }} / create_table
#
# ## \[1- config\] get task configuration as defined in dag_specs.yaml

# <codecell>

# Get task configuration, as defined in dag_specs.yaml
task_config = dsflow.get_config(flow_name="{{ flow_name }}",
                                task_name="create_table",
                                notebook_ds="newest")

print(task_config)

# <codecell>

# Get task variables:
source_path = task_config["source_path"]
sink_path = task_config["sink_path"]
ds = task_config["ds"]  # ds is the execution date

# <markdowncell>

# ## \[2- first data check\] using Spark, print a couple lines from source_path

# <codecell>

raw_data = spark.read.text(source_path)

if raw_data.count() == 0:
    raise Exception("no data found")

for line in raw_data.take(20):
    print(line.value)

# <markdowncell>

## \[3- process data\] convert raw data into a Spark dataframe


# <codecell>

dataset = (spark
.read
.options(header=True, wholeFile=True, inferSchema=True, delimiter=',')
.csv(source_path)
)

# <markdowncell>

# number of rows in dataset

# <codecell>

dataset.count()

# <codecell>

# by default, dataframe uses an auto-detected schema
dataset.printSchema()

# <markdowncell>

# ## First 10 rows

# <codecell>

dsflow.display(dataset)

# <codecell>

dsflow.display(dataset.describe())

# <markdowncell>

# ## \[4- write on disk\] write down the output to disk

# <codecell>

final_dataset = dataset

# <codecell>

(final_dataset
 .write
 .mode("overwrite")
 .parquet(sink_path)
)
