# <markdowncell>

# *Note: this notebook was generated by dsflow*
#
# # create table {{ dataset_name }}
#
# ## \[1- init and config\] refresh tables and set output path

# <codecell>

import os
import json
import datetime as dt
from pyspark.sql import SparkSession
from dsflow_core.helpers import DsflowContext

spark = SparkSession.builder.getOrCreate()
dsflow = DsflowContext.create()

spark

# <codecell>

dsflow.load_tables()

# <codecell>

default_task_specs = """
    {"sink_path": "/data/tables/{{ dataset_name }}/ds={{ ds }}",
     "ds": "{{ ds }}"}
    """

# <codecell>

task_specs_raw = os.environ.get('TASK_SPECS', default_task_specs)
task_specs = json.loads(task_specs_raw)

# dsflow alerts if something looks wrong
dsflow.validade_task_specs(task_specs)
print(task_specs)

# <markdowncell>


# ## \[2- process data\] process data based on SQL

# <codecell>

df = spark.sql("""

  SELECT *
  FROM my_source_table
  WHERE ds = '{{ ds }}'

""")

# <markdowncell>

# Execute query and show first 10 rows

# <codecell>

dsflow.display(df)

# <markdowncell>

# HINT: Register dataframe as a temporary view

# <codecell>

df.registerTempTable("tmp_{{ dataset_name }}")

# <codecell>

other_df = spark.sql("""

  SELECT COUNT(*)
  FROM tmp_{{ dataset_name }}

""")

dsflow.display(other_df)

# <codecell>

# print schema
df.printSchema()


# <markdowncell>

# ## \[3- write on disk\] write the output as parquet

# <codecell>

final_dataset = df

# <codecell>

(final_dataset
 .write
 .mode("overwrite")
 .parquet(task_specs["sink_path"])
)

# <codecell>

dsflow.validade_task_output(task_specs)
