# <markdowncell>

# *Note: this notebook was generated by dsflow*
#
# # create table {{ dataset_name }}
#
# ## \[1- config\] set input and output paths

# <codecell>

import os
import json
import datetime as dt
from pyspark.sql import SparkSession
from dsflow_core.helpers import DsflowContext

spark = SparkSession.builder.getOrCreate()
dsflow = DsflowContext.create()

spark

# <codecell>

default_task_specs = """
    {"source_path": "/data/raw/{{ dataset_name }}/ds={{ ds }}",
     "sink_path": "/data/tables/{{ dataset_name }}/ds={{ ds }}",
     "ds": "{{ ds }}"}
    """

# <codecell>

task_specs_raw = os.environ.get('TASK_SPECS', default_task_specs)
task_specs = json.loads(task_specs_raw)

# dsflow alerts if something looks wrong
dsflow.validade_task_specs(task_specs)

print(task_specs)

# <codecell>

# Get task variables:
source_path = task_specs["source_path"]
sink_path = task_specs["sink_path"]
ds = task_specs["ds"]  # ds is the execution date

# <codecell>

# https://public.opendatasoft.com/explore/dataset/donnees-synop-essentielles-omm/information/

# <markdowncell>

# ## \[2- first data check\] using Spark, print a couple lines from source_path

# <codecell>

raw_data = spark.read.text(source_path)

# <markdowncell>

# ## \[3- process data\] convert raw data into a Spark dataframe

# <codecell>

dataset = (spark
.read
.options(wholeFile=True, inferSchema=True)
.json(source_path)
)

# <markdowncell>

# number of rows in dataset

# <codecell>

dataset.count()

# <codecell>

# by default, dataframe uses an auto-detected schema
dataset.printSchema()

# <codecell>

# by default, dataframe uses an auto-detected schema
dataset.printSchema()

# <markdowncell>

# ## First 10 rows

# <codecell>

dsflow.display(dataset)

# <codecell>

dsflow.display(dataset.describe())

# <markdowncell>

# ## \[4- write on disk\] write down the output to disk

# <codecell>

final_dataset = dataset

# <codecell>

(final_dataset
 .write
 .mode("overwrite")
 .parquet(sink_path)
)

# <codecell>

dsflow.validade_task_output(task_specs)
