# <codecell>

import os
import json
import datetime as dt
from pprint import *
from pyspark.sql import SparkSession
from dsflow_core.helpers import DsflowContext

spark = SparkSession.builder.getOrCreate()
dsflow = DsflowContext.create()

spark

# <codecell>

default_task_specs = """
    {"source_path": "{{ source_path }}",
     "sink_path": "/data/tables/{{ dataset_name }}/ds={{ ds }}",
     "ds": "{{ ds }}"}
    """

# <codecell>

task_specs_raw = os.environ.get('TASK_SPECS', default_task_specs)
task_specs = json.loads(task_specs_raw)

# dsflow alerts if something looks wrong
dsflow.validade_task_specs(task_specs)

pprint(task_specs)

# <codecell>

from pyspark.sql.types import StructType
import yaml

yaml_schema_path = "{{ schema_path }}"
df_schema = StructType.fromJson(yaml.load(open(yaml_schema_path, 'r')))
{{ dataset_name }}_df = spark.read.schema(df_schema).options().json(task_specs["source_path"])
{{ dataset_name }}_df.printSchema()

# <codecell>

# {{ dataset_name }}_df = json_data.selectExpr("explode({{ dataset_name }}) as item").selectExpr("item.*")
# {{ dataset_name }}_df.printSchema()

# <codecell>

{{ dataset_name }}_df.limit(10).toPandas()

# <codecell>

{{ dataset_name }}_df.write.parquet(task_specs["sink_path"])

# <codecell>
